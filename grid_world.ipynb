{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subclassing gymnasium.Env\n",
    "\n",
    "This environment implements a very simplistic game consisting of a 2-dimensional square grid of fixed size (specified via the size parameter during construction). The agent can move vertically or horizontally between grid cells in each timestep. The goal of the agent is to navigate to a target on the grid that has been placed randomly at the beginning of the episode.\n",
    "\n",
    "- States (observations) provide the location of the target and agent.\n",
    "- There are 4 actions in our environment, corresponding to the movements “right”, “up”, “left”, and “down”.\n",
    "- A done signal is issued as soon as the agent has navigated to the grid cell where the target is located.\n",
    "- Rewards are binary and sparse, meaning that the immediate reward is always zero, unless the agent has reached the target, then it is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3.dev8 (SDL 2.0.22, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration and Initialization\n",
    "\n",
    "Our custom environment will inherit from the abstract class `gymnasium.Env`. You shouldn’t forget to add the metadata attribute to your class. There, you should specify the render-modes that are supported by your environment (e.g. `\"human\"`, `\"rgb_array\"`, `\"ansi\"`) and the framerate at which your environment should be rendered. Every environment should support `None` as render-mode; you don’t need to add it in the metadata. In `GridWorldEnv`, we will support the modes “rgb_array” and “human” and render at 4 FPS.\n",
    "\n",
    "The `__init__` method of our environment will accept the integer `size`, that determines the size of the square grid. We will set up some variables for rendering and define `self.observation_space` and `self.action_space`. In our case, observations should provide information about the location of the agent and target on the 2-dimensional grid. We will choose to represent observations in the form of dictionaries with keys `\"agent\"` and `\"target\"`. An observation may look like `{\"agent\": array([1, 0]), \"target\": array([0, 3])}`. Since we have 4 actions in our environment (“right”, “up”, “left”, “down”), we will use `Discrete(4)` as an action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\n",
    "        \"render_models\": [\"human\", \"rgb_array\"], \n",
    "        \"render_fps\": 4\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size # size of the square grid\n",
    "        self.window_size = 512 # size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # A location is encoded as a tuple (x, y) of ints in {0,...,'size'-1}, i.e. MultiDiscrete([size, size]).\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from 'self.action_space' to the direction we will walk in \n",
    "        if that action is taken. I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0, 1, 2, 3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use 'np.clip' to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(self._agent_location + direction, 0, self.size - 1)\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location)\n",
    "        reward = 1 if terminated else 0 # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28c168c855826d6a306c7ef584e2992a063631e8777795e1e4ec8bd8ff15ed8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
